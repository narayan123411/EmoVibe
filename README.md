# EmoVibe
EmoVibe: Personalized Emotion-Driven Music Recommendations

Project Leader: Narayan Raval

Project Member: Benjamin Claudius, Mounika katta

# Aim: 
The aim of this project is to develop an intelligent AI-driven music companion, EmoVibe, capable of curating and playing music in real-time based on the user's emotions and situational context. This system will leverage advanced emotion recognition and contextual understanding techniques to accurately identify a user's emotions, whether expressed through text or voice, and cater to their current situation, such as driving or relaxing. By analyzing emotional cues and situational information, EmoVibe will provide a seamless music experience that aligns with the user's emotional state and activities, enhancing their mood and overall well-being.

# Introduction: 

Introducing " EmoVibe " – your ultimate AI music buddy. It tunes in to how you feel and what you're up to, picking just the right music for you. Whether you're a bit down or on a road trip, EmoVibe picks music that fits your vibe. This clever friend is all about boosting your mood and changing how music makes you feel. It's like having your own personal DJ that understands you perfectly, making your music experience all about you.

# Methodology:

1.	Gathering User Preferences:
•	Utilization Approach: Initiate the user journey by collecting insights into their musical tastes and preferred genres.
•	Implementation: Design an engaging questionnaire to inquire about the user's preferred music genres, allowing the system to tailor recommendations accordingly.

2.	Emotion Recognition and Contextual Understanding:
•	Utilization Approach: Enable the system to understand the user's emotions and contextual activities through text and voice inputs.
•	Implementation: Employ advanced sentiment analysis models (VADER, TextBlob) for text inputs, and deploy sophisticated deep learning models (LSTM, Transformers) for voice inputs. Enhance context comprehension using keyword analysis for activities like driving, gym, and more.

3.	Music Analysis and Emotional Mapping:
•	Utilization Approach: Leverage music analysis to categorize songs based on emotions, aligning them with user sentiments.
•	Implementation: Utilize audio feature extraction libraries such as Librosa to extract emotional attributes like tempo and key. Incorporate NLP techniques to analyze song lyrics for emotional cues. Create algorithms that map recognized emotions, contextual data, and genre preferences to suitable emotional attributes of songs.
4.	User Interface and Interaction Enhancement:
•	Utilization Approach: Elevate user engagement by offering an intuitive interface for emotion and context input, while also gathering genre preferences.
•	Implementation: Optimize the user interface design to facilitate seamless emotion and context capture through text and voice. Integrate Google Colab for building and training the emotion recognition model. Incorporate the Speech Recognition library for accurate voice input.

5.	Tailored Music Recommendations:
•	Utilization Approach: Deliver personalized music recommendations that resonate with the user's mood, activities, and preferred genres.
•	Implementation: Craft a recommendation system that aligns recognized emotions, context, and genre preferences. Utilize content-based approaches by analyzing emotional attributes and genre classifications of songs. Implement collaborative filtering to match user context and preferences with similar profiles.

# Conclusion:

An innovation in applying AI to personalize music encounters is the Emotion-Driven Music Companion project. The technology recommends music that strongly connects by automatically analyzing emotions and context and pairing them with user choices. Each user has a distinctive and captivating musical journey because of the seamless integration of sentiment analysis, contextual awareness, and genre choices. The potential for this initiative to completely transform how we interact with music as technology develops is limitless.

# Future Work:

•	Fine-Tuned Emotion Recognition: Continuously refining emotion recognition models for more precise results.
•	Global Reach: Expanding language and cultural support to cater to diverse audiences.
•	Real-Time Adaptation: Integrating real-time data for immediate context-based recommendations.
•	Learning and Feedback: Incorporating machine learning to learn from user interactions and enhance suggestions.
